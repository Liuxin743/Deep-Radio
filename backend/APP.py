# backend/app.py
from flask import Flask, jsonify
from flask_cors import CORS
import requests
import re
from datetime import datetime
import json
import os
import sys

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

class LLMAgentsCrawler:
    def __init__(self):
        self.repo_owner = "WooooDyy"
        self.repo_name = "LLM-Agent-Paper-List"
        self.base_url = f"https://api.github.com/repos/{self.repo_owner}/{self.repo_name}"
        self.cache_file = os.path.join(current_dir, "llm_agents_cache.json")
        self.cache_duration = 3600
        
    def get_readme_content(self):
        """è·å–READMEå†…å®¹"""
        try:
            print("æ­£åœ¨ä»GitHubè·å–æ•°æ®...")
            url = f"{self.base_url}/readme"
            headers = {
                'Accept': 'application/vnd.github.v3+json',
                'User-Agent': 'LLM-Agent-News-Crawler'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                content = response.json()['content']
                import base64
                return base64.b64decode(content).decode('utf-8')
            else:
                print(f"GitHub APIå“åº”çŠ¶æ€ç : {response.status_code}")
                return None
        except Exception as e:
            print(f"è·å–READMEå¤±è´¥: {e}")
            return None
    
    def parse_news_section(self, readme_content):
        """è§£ææ–°é—»å…¬å‘Šéƒ¨åˆ†"""
        try:
            print("æ­£åœ¨è§£ææ–°é—»éƒ¨åˆ†...")
            news_section = re.search(r'## ğŸ”” News\s*\n(.*?)(?=\n## \w|$)', readme_content, re.DOTALL)
            if not news_section:
                print("æœªæ‰¾åˆ°æ–°é—»éƒ¨åˆ†")
                return []
                
            news_text = news_section.group(1)
            news_items = []
            lines = news_text.split('\n')
            current_news = ""
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                if re.match(r'^[ğŸ‰ğŸºğŸš€ğŸ‘€â˜„ï¸ğŸ’«ğŸ¥³ğŸ’¥âœ¨]', line):
                    if current_news:
                        parsed_news = self.parse_single_news(current_news)
                        if parsed_news:
                            news_items.append(parsed_news)
                    current_news = line
                elif current_news:
                    current_news += " " + line
            
            if current_news:
                parsed_news = self.parse_single_news(current_news)
                if parsed_news:
                    news_items.append(parsed_news)
                    
            print(f"è§£æåˆ° {len(news_items)} æ¡æ–°é—»")
            return news_items
        except Exception as e:
            print(f"è§£ææ–°é—»å¤±è´¥: {e}")
            return []
    
    def parse_single_news(self, news_text):
        """è§£æå•æ¡æ–°é—»"""
        try:
            date_match = re.search(r'\[(\d{4}/\d{2}/\d{2})\]', news_text)
            date = date_match.group(1).replace('/', '-') if date_match else None
            
            emoji_match = re.match(r'([ğŸ‰ğŸºğŸš€ğŸ‘€â˜„ï¸ğŸ’«ğŸ¥³ğŸ’¥âœ¨])\s*(.*)', news_text)
            emoji = emoji_match.group(1) if emoji_match else "ğŸ“¢"
            content = emoji_match.group(2) if emoji_match else news_text
            
            content = re.sub(r'\[.*?\]\(.*?\)', '', content).strip()
            
            paper_links = re.findall(r'\[paper\]\((.*?)\)', news_text)
            code_links = re.findall(r'\[code\]\((.*?)\)', news_text)
            project_links = re.findall(r'\[project page\]\((.*?)\)', news_text)
            
            return {
                'date': date,
                'emoji': emoji,
                'content': content,
                'links': {
                    'paper': paper_links[0] if paper_links else None,
                    'code': code_links[0] if code_links else None,
                    'project': project_links[0] if project_links else None
                },
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            print(f"è§£æå•æ¡æ–°é—»å¤±è´¥: {e}")
            return None
    
    def get_recent_papers(self, readme_content, limit=10):
        """è·å–æœ€æ–°è®ºæ–‡"""
        try:
            print("æ­£åœ¨è§£æè®ºæ–‡éƒ¨åˆ†...")
            papers = []
            paper_pattern = r'\[(\d{4}/\d{2}/\d{2})\]\s*(.*?)\s*\.\s*([^\.\n]+?)(?:\.\s*)?(\[paper\]\(.*?\))(?:\s*\[code\]\((.*?)\))?(?:\s*\[project page\]\((.*?)\))?'
            matches = re.findall(paper_pattern, readme_content)
            
            for match in matches:
                paper = {
                    'date': match[0].replace('/', '-'),
                    'title': match[1].strip(),
                    'authors': match[2].strip(),
                    'paper_link': re.search(r'\((.*?)\)', match[3]).group(1) if match[3] else None,
                    'code_link': re.search(r'\((.*?)\)', match[4]).group(1) if match[4] else None,
                    'project_link': re.search(r'\((.*?)\)', match[5]).group(1) if match[5] else None
                }
                papers.append(paper)
                
            print(f"è§£æåˆ° {len(papers)} ç¯‡è®ºæ–‡")
            return sorted(papers, key=lambda x: x['date'], reverse=True)[:limit]
        except Exception as e:
            print(f"è§£æè®ºæ–‡å¤±è´¥: {e}")
            return []
    
    def is_cache_valid(self):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(self.cache_file):
            return False
        file_time = os.path.getmtime(self.cache_file)
        return (datetime.now().timestamp() - file_time) < self.cache_duration
    
    def load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return None
    
    def save_cache(self, data):
        """ä¿å­˜ç¼“å­˜"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def crawl_all_data(self):
        """çˆ¬å–æ‰€æœ‰æ•°æ®"""
        if self.is_cache_valid():
            cached_data = self.load_cache()
            if cached_data:
                print("ä½¿ç”¨ç¼“å­˜æ•°æ®")
                return cached_data
        
        print("å¼€å§‹çˆ¬å–æ–°æ•°æ®...")
        readme_content = self.get_readme_content()
        if not readme_content:
            print("æ— æ³•è·å–READMEå†…å®¹")
            return None
            
        data = {
            'news': self.parse_news_section(readme_content),
            'papers': self.get_recent_papers(readme_content),
            'last_updated': datetime.now().isoformat(),
            'source': 'https://github.com/WooooDyy/LLM-Agent-Paper-List'
        }
        
        self.save_cache(data)
        print("æ•°æ®çˆ¬å–å®Œæˆ!")
        return data

# åˆ›å»ºçˆ¬è™«å®ä¾‹
crawler = LLMAgentsCrawler()

@app.route('/')
def home():
    return '''
    <h1>LLM Agents å‰æ²¿åŠ¨æ€ API</h1>
    <p>æœåŠ¡æ­£åœ¨è¿è¡Œï¼</p>
    <p>å¯ç”¨æ¥å£ï¼š</p>
    <ul>
        <li><a href="/api/data">/api/data</a> - è·å–æ•°æ®</li>
        <li><a href="/api/refresh">/api/refresh</a> - åˆ·æ–°æ•°æ®</li>
    </ul>
    '''

@app.route('/api/data')
def get_data():
    """è·å–LLM Agentsæ•°æ®API"""
    try:
        print("æ¥æ”¶åˆ°æ•°æ®è¯·æ±‚...")
        data = crawler.crawl_all_data()
        if data:
            return jsonify({
                'success': True,
                'data': data
            })
        else:
            return jsonify({
                'success': False,
                'error': 'æ— æ³•è·å–æ•°æ®'
            })
    except Exception as e:
        print(f"APIé”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/refresh')
def refresh_data():
    """å¼ºåˆ¶åˆ·æ–°æ•°æ®"""
    try:
        print("æ¥æ”¶åˆ°åˆ·æ–°è¯·æ±‚...")
        if os.path.exists(crawler.cache_file):
            os.remove(crawler.cache_file)
            print("ç¼“å­˜æ–‡ä»¶å·²åˆ é™¤")
        
        data = crawler.crawl_all_data()
        if data:
            return jsonify({
                'success': True,
                'message': 'æ•°æ®åˆ·æ–°æˆåŠŸ',
                'data': data
            })
        else:
            return jsonify({
                'success': False,
                'error': 'åˆ·æ–°æ•°æ®å¤±è´¥'
            })
    except Exception as e:
        print(f"åˆ·æ–°é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

if __name__ == '__main__':
    print("æ­£åœ¨å¯åŠ¨LLM Agents APIæœåŠ¡...")
    print("æœåŠ¡å°†åœ¨ http://127.0.0.1:5001 å¯åŠ¨")
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    app.run(
        debug=True, 
        host='127.0.0.1', 
        port=5001,
        threaded=True
    )